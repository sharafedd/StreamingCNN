{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StreamingCNN\n",
    "\n",
    "To train deep convolutional neural networks, the input data and the activations need to be kept in memory. Given the limited memory available in current GPUs, this limits the maximum dimensions of the input data. StreamingCNN allows for training a convolutional neural networks while holding only parts of the image in memory. \n",
    "\n",
    "**This notebook shows numerical equivalence to a conventional forward and backward pass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.384174Z",
     "start_time": "2019-11-08T15:07:35.663176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharaf/Projects/scnn/StreamingCNN/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/sharaf/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /home/sharaf/.cache/torch_extensions/cpp_functions/build.ninja...\n",
      "Building extension module cpp_functions...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpp_functions...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scnn import StreamingCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.389332Z",
     "start_time": "2019-11-08T15:07:36.385829Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize an small example network here. All layers are supported, except for feature-wide operations (BatchNormalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.406848Z",
     "start_time": "2019-11-08T15:07:36.391150Z"
    }
   },
   "outputs": [],
   "source": [
    "padding = 0\n",
    "\n",
    "stream_net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enlarge the weights a bit to increase the gradient sizes (better for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.411882Z",
     "start_time": "2019-11-08T15:07:36.408857Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        layer.weight.data *= 2.5\n",
    "        layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.418976Z",
     "start_time": "2019-11-08T15:07:36.414013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (8): ReLU()\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (11): ReLU()\n",
      "  (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (13): ReLU()\n",
      "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(stream_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.431330Z",
     "start_time": "2019-11-08T15:07:36.422710Z"
    }
   },
   "outputs": [],
   "source": [
    "tile_size = 512\n",
    "img_size = 1024\n",
    "\n",
    "cuda = True  # execute this notebook on the GPU\n",
    "verbose = True   # enable / disable logging\n",
    "dtype = torch.double  # test with double precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:39.681141Z",
     "start_time": "2019-11-08T15:07:36.434031Z"
    }
   },
   "outputs": [],
   "source": [
    "stream_net.type(dtype)\n",
    "if cuda: stream_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure StreamingCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#FF000'>**IMPORTANT:**</font> setting ```gather_gradients``` to ```True``` makes the class save all the gradients of the intermediate feature maps. This is needed because we want to compare the feature map gradients between streaming and conventional backpropagation. However this also counteracts the memory gains by StreamingCNN. If you want to test the memory efficiency, set ```gather_gradients``` to ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:40.502606Z",
     "start_time": "2019-11-08T15:07:39.683395Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "\n",
      " Output lost Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " Lost(top:0.0, left:0.0, bottom:0.0, right:0.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:0.0, left:0.0, bottom:1.0, right:1.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:2.0, left:2.0, bottom:3.0, right:3.0)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " Lost(top:4.0, left:4.0, bottom:5.0, right:5.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:8.0, left:8.0, bottom:10.0, right:10.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:10.0, left:10.0, bottom:12.0, right:12.0)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " Lost(top:12.0, left:12.0, bottom:14.0, right:14.0)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:24.0, left:24.0, bottom:28.0, right:28.0)\n",
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " Lost(top:26.0, left:26.0, bottom:30.0, right:30.0)\n",
      "\n",
      " Input gradient lost Lost(top:28.0, left:28.0, bottom:32.0, right:32.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharaf/Projects/scnn/StreamingCNN/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "sCNN = StreamingCNN(stream_net, \n",
    "                    tile_shape=(1, 3, tile_size, tile_size), \n",
    "                    verbose=True, \n",
    "                    gather_gradients=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ```verbose``` flag is ```True``` than StreamingCNN will print for every layer in the network the required overlap that is needed to reconstruct the feature maps and gradients. The higher this is, the more tiles are needed to be inferences. It is always beneficial to increase the tile size as much as possible to make use of all the GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate random image and fake label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:40.550783Z",
     "start_time": "2019-11-08T15:07:40.504817Z"
    }
   },
   "outputs": [],
   "source": [
    "image = torch.FloatTensor(3, img_size, img_size).normal_(0, 1)\n",
    "target = torch.tensor(50.)  # large value so we get larger gradients\n",
    "\n",
    "image = image.type(dtype)\n",
    "target = target.type(dtype)\n",
    "\n",
    "if cuda:\n",
    "    target = target.cuda()\n",
    "    image = image.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:40.554447Z",
     "start_time": "2019-11-08T15:07:40.552045Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through network using streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.253803Z",
     "start_time": "2019-11-08T15:07:40.555823Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharaf/Projects/scnn/StreamingCNN/venv/lib/python3.8/site-packages/torch/_tensor.py:579: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(other, self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles in forward: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 513.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.9242553953, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if image.dim() == 3:\n",
    "    image = image.unsqueeze(0)\n",
    "stream_output = sCNN.forward(image); stream_output.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.257403Z",
     "start_time": "2019-11-08T15:07:41.255144Z"
    }
   },
   "outputs": [],
   "source": [
    "stream_output.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.263939Z",
     "start_time": "2019-11-08T15:07:41.259009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7766771896, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.sigmoid(torch.mean(stream_output)); output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.270191Z",
     "start_time": "2019-11-08T15:07:41.265537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-60.8211881103, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(output, target); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.274133Z",
     "start_time": "2019-11-08T15:07:41.271619Z"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.883831Z",
     "start_time": "2019-11-08T15:07:41.275420Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401.3333333333333 9 572.0\n",
      "Number of tiles in backprop: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 126.98it/s]\n"
     ]
    }
   ],
   "source": [
    "full_gradients = sCNN.backward(image, stream_output.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.888465Z",
     "start_time": "2019-11-08T15:07:52.885712Z"
    }
   },
   "outputs": [],
   "source": [
    "sCNN.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the gradients of the Conv2d layer to compare with the conventional method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.894909Z",
     "start_time": "2019-11-08T15:07:52.890361Z"
    }
   },
   "outputs": [],
   "source": [
    "streaming_conv_gradients = []\n",
    "\n",
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.weight.grad is not None:\n",
    "            streaming_conv_gradients.append(layer.weight.grad.clone()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to conventional method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the gradients and add hooks to the network to gather the gradients of the intermediate feature maps to compare with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.914145Z",
     "start_time": "2019-11-08T15:07:52.899281Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.weight.grad is not None:\n",
    "            layer.weight.grad.data.zero_()\n",
    "            layer.bias.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.933620Z",
     "start_time": "2019-11-08T15:07:52.927035Z"
    }
   },
   "outputs": [],
   "source": [
    "conventional_gradients = []\n",
    "inps = []\n",
    "\n",
    "def save_grad(module, grad_in, grad_out):\n",
    "    global conventional_gradients\n",
    "    conventional_gradients.append(grad_out[0].clone())\n",
    "        \n",
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        layer.register_backward_hook(save_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output should be the same as the streaming output, if so, the loss will also be the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.259202Z",
     "start_time": "2019-11-08T15:07:52.942475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9242553953, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conventional_output = stream_net(image); conventional_output.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.274730Z",
     "start_time": "2019-11-08T15:07:53.265669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal output to streaming\n"
     ]
    }
   ],
   "source": [
    "# NOTE: sometimes output can be slightly bigger \n",
    "# (if tiles do not fit nicely on input image according to output stride)\n",
    "# In that case this check may fail.\n",
    "max_error = torch.abs(stream_output - conventional_output).max().item()\n",
    "\n",
    "if max_error < 1e-7:\n",
    "    print(\"Equal output to streaming\")\n",
    "else:\n",
    "    print(\"NOT equal output to streaming\"),\n",
    "    print(\"error:\", max_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.292242Z",
     "start_time": "2019-11-08T15:07:53.283516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7766771896, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.sigmoid(torch.mean(conventional_output)); output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.304973Z",
     "start_time": "2019-11-08T15:07:53.299766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-60.8211881103, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(output, target); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.314329Z",
     "start_time": "2019-11-08T15:07:53.306628Z"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the gradients of the feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell concatenates all the intermediate feature map gradients of the tiles to compare it to the feature map gradients calculated during the conventional method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:09:28.505689Z",
     "start_time": "2019-11-08T15:09:28.434644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No suitable Conv2d layer with 9 tiles found in sCNN.gradients.\n"
     ]
    }
   ],
   "source": [
    "equal_eps = 1e-17  # because we are comparing floats the difference is almost never exactly 0\n",
    "\n",
    "# Find the first Conv2d layer that exists in sCNN.gradients\n",
    "conv_layer = None\n",
    "for m in stream_net.modules():\n",
    "    if isinstance(m, torch.nn.Conv2d) and m in sCNN.gradients:\n",
    "        conv_layer = m\n",
    "        break\n",
    "\n",
    "if conv_layer is not None and len(sCNN.gradients[conv_layer]) == 9:\n",
    "\n",
    "    layer_dict = dict(stream_net.named_modules())\n",
    "    i = -1\n",
    "    for name in layer_dict:\n",
    "        mod = layer_dict[name]\n",
    "        if isinstance(mod, torch.nn.Conv2d) and mod in sCNN.gradients and len(name) > 0:\n",
    "            i += 1\n",
    "\n",
    "            # StreamingCNN streams from top-left to bottom-right. \n",
    "            # First concat the columns, then the rows:\n",
    "            a = torch.cat((sCNN.gradients[mod][0][0], \n",
    "                           sCNN.gradients[mod][1][0], \n",
    "                           sCNN.gradients[mod][2][0]), dim=2)\n",
    "            b = torch.cat((sCNN.gradients[mod][3][0], \n",
    "                           sCNN.gradients[mod][4][0], \n",
    "                           sCNN.gradients[mod][5][0]), dim=2)\n",
    "            c = torch.cat((sCNN.gradients[mod][6][0], \n",
    "                           sCNN.gradients[mod][7][0], \n",
    "                           sCNN.gradients[mod][8][0]), dim=2)\n",
    "            \n",
    "            str_grad = torch.cat((a, b, c), dim=1)\n",
    "\n",
    "            # Compare streaming and conventional:\n",
    "            max_error = torch.abs(str_grad - conventional_gradients[-(i + 1)][0])\n",
    "            max_error = max_error.max().item()\n",
    "            \n",
    "            if max_error < equal_eps:\n",
    "                print(name, \"feature map gradient - equal to non-streaming\")\n",
    "            else:\n",
    "                print(name, \"feature map - NOT equal, max error:\", max_error)\n",
    "else:\n",
    "    print(\"No suitable Conv2d layer with 9 tiles found in sCNN.gradients.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the gradients of the conv2d layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the gradients of the conv2d layer to compare with normal SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.911329Z",
     "start_time": "2019-11-08T15:07:54.905167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv layer 0 \t Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 1 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 2 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 3 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 4 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 5 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "normal_conv_gradients = []\n",
    "j = 0\n",
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.weight.grad is not None:\n",
    "            normal_conv_gradients.append(layer.weight.grad) \n",
    "            print('Conv layer', j, '\\t', layer)\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.929868Z",
     "start_time": "2019-11-08T15:07:54.913460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Conventional', '\\n')\n",
    "\n",
    "for i in range(len(streaming_conv_gradients)):\n",
    "    print(\"Conv layer\", i, \"\\t average gradient size:\", \n",
    "          float(torch.mean(torch.abs(streaming_conv_gradients[i].data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.962148Z",
     "start_time": "2019-11-08T15:07:54.934375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming \n",
      "\n",
      "Conv layer 0 \t average gradient size: 0.5335104974390105\n",
      "Conv layer 1 \t average gradient size: 1.0143260372301737\n",
      "Conv layer 2 \t average gradient size: 2.00521271720441\n",
      "Conv layer 3 \t average gradient size: 0.8939830323267149\n",
      "Conv layer 4 \t average gradient size: 2.268768575334102\n",
      "Conv layer 5 \t average gradient size: 1.9859810559858897\n"
     ]
    }
   ],
   "source": [
    "print('Streaming', '\\n')\n",
    "for i in range(len(normal_conv_gradients)):\n",
    "    print(\"Conv layer\", i, \"\\t average gradient size:\", \n",
    "          float(torch.mean(torch.abs(normal_conv_gradients[i].data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.976045Z",
     "start_time": "2019-11-08T15:07:54.969652Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(streaming_conv_gradients)):\n",
    "    diff = torch.abs(streaming_conv_gradients[i].data - normal_conv_gradients[i].data)\n",
    "    max_diff = diff.max()\n",
    "    print(\"Conv layer\", i, \"\\t max difference between kernel gradients:\", \n",
    "          float(max_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the difference of the gradients of the conv2d layers between the methods is (almost) numerically equivalent. The small differences are because of loss of significance with the floating points calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "344px",
    "left": "2614.95px",
    "top": "98.0799px",
    "width": "383px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
